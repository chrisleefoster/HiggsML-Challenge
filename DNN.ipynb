{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATA #\n",
    "import numpy as np\n",
    "\n",
    "# features consists of the EventID and the 30 features\n",
    "features = np.loadtxt(\"atlas-higgs-challenge-2014-v2.csv\", delimiter=\",\", skiprows=1, usecols=range(0,31))\n",
    "\n",
    "# labels consists of the EventID, a weight, and a label\n",
    "# the weights are used for the unnormalized true positives and false positive rates in the AMS calculation\n",
    "labels = np.loadtxt(\"atlas-higgs-challenge-2014-v2.csv\", delimiter=\",\", skiprows=1, usecols=(0,31,32), dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out what the data looks like\n",
    "print(features[0])\n",
    "print(labels[0])\n",
    "print(labels.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize feature values\n",
    "# scale all but first column (EventID)\n",
    "scaler = StandardScaler().fit(features[:,1:])\n",
    "temp_features = scaler.transform(features[:,1:])\n",
    "\n",
    "# link EventIDs with scaled features\n",
    "scaled_features = features\n",
    "for sample in range(scaled_features.shape[0]):\n",
    "    scaled_features[sample, 1:] = temp_features[sample]\n",
    "\n",
    "# check out how it looks now\n",
    "print(scaled_features[1])\n",
    "print(scaled_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split to test and train sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(scaled_features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the split data\n",
    "print(features_train[0])\n",
    "print(labels_train[0])\n",
    "print(features_test.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create solution submission for AMS metric\n",
    "import csv\n",
    "\n",
    "# the provided AMS calculation requires a csv file with each sample's: EventID, Class, Weight\n",
    "with open('solution.csv', 'w', newline='') as solution_file:\n",
    "    headings = ['EventId', 'Class', 'Weight']\n",
    "    writer = csv.DictWriter(solution_file, fieldnames=headings)\n",
    "    writer.writeheader()\n",
    "    for sample in range(labels_test.shape[0]):\n",
    "        writer.writerow({\"EventId\": labels_test[sample,0], \"Class\": labels_test[sample,2], \"Weight\": labels_test[sample,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change labels from string to binary\n",
    "for label in range(labels_train.shape[0]):\n",
    "    if labels_train[label,-1] == 'b':\n",
    "        labels_train[label,-1] = 0\n",
    "    else:\n",
    "        labels_train[label,-1] = 1\n",
    "\n",
    "for label in range(labels_test.shape[0]):\n",
    "    if labels_test[label,-1] == 'b':\n",
    "        labels_test[label,-1] = 0\n",
    "    else:\n",
    "        labels_test[label,-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data again\n",
    "print(labels_test[0])\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(30,)))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0005),     # optimization method\n",
    "                   loss=tf.keras.losses.BinaryCrossentropy())              # loss function to be minimized (Binary Cross-entropy)\n",
    "\n",
    "# this line is used for optimizing the number of epochs used in training\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss',           # stop training when the validation loss does not improve\n",
    "                                              patience=5,                   # give the model 5 epochs to improve\n",
    "                                              restore_best_weights=True)    # save the weights that produced the minimum validation loss\n",
    "\n",
    "# train the model\n",
    "trained_model = model.fit(features_train[:,1:], np.asarray(labels_train[:,-1].astype('int')).reshape((-1,1)), # training data                \n",
    "                          validation_split=0.2,       # save 20% of data for validation\n",
    "                          epochs=100,                 # maximum number of epochs\n",
    "                          batch_size=256,             # batch size\n",
    "                          callbacks = [stop_early])   # early stopping\n",
    "\n",
    "# predict the labels of the test features\n",
    "predictions = model.predict(features_test[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot the loss of the training and validation set \n",
    "plt.plot(trained_model.history['loss'], label='loss')\n",
    "plt.plot(trained_model.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change labels back to char, 's' or 'b'\n",
    "labels_pred = [0] * predictions.shape[0]\n",
    "for label in range(predictions.shape[0]):\n",
    "    if predictions[label] < 0.5:\n",
    "        labels_pred[label] = 'b'\n",
    "    else:\n",
    "        labels_pred[label] = 's'\n",
    "print(labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediction submission for AMS metric calculation\n",
    "with open('submission.csv', 'w', newline='') as submission_file:\n",
    "    headings = ['EventId', 'RankOrder', 'Class']\n",
    "    writer = csv.DictWriter(submission_file, fieldnames=headings)\n",
    "    writer.writeheader()\n",
    "    for sample in range(labels_test.shape[0]):\n",
    "        writer.writerow({\"EventId\": labels_test[sample,0], \"RankOrder\": sample, \"Class\": labels_pred[sample]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AMS\n",
    "import HiggsBosonCompetition_AMSMetric_rev1 as ams\n",
    "ams.AMS_metric(\"solution.csv\", \"submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
